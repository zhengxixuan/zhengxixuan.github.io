---
title: 大模型训练生态
description: 盘点近三年LLM训练的生态，包括开源和商业。
date: 2025-07-09 08:30:00 +0800
categories: [AI, training]
tags: [thinking]
---

自2022年11月ChatGPT发布以来，大语言模型（LLM）领域在短短不到三年时间，经历了爆炸式的发展，仅是 OpenAI 一家公司的故事都足以写成一本厚厚的传记了。


**在这个大模型时代，谁能训练出能力更强的模型，谁就掌握了主导权。**两年前的初创公司 OpenAI 和 Anthropic 如今已成为大模型领域的前沿巨头，而原本互联网时代的巨头 Google、Meta 也不甘落于人后，投入大量资金启动模型研发。


**本文主要对当前LLM的全球格局做一次系统性梳理，帮助了解大语言模型训练生态。**


**LLM的训练需要巨大的计算能力、海量的高质量数据和顶尖的人才储备。**目前LLM训练生态系统呈现出多元化的发展态势，既有大型科技公司主导的闭源模型研发，也有由研究机构和初创公司推动的开源和去中心化浪潮。

理解这个生态系统的构成、主要参与者的策略以及技术发展趋势，对于把握人工智能的未来走向至关重要。不同的训练方法、数据策略和模型架构（如Transformer、MoE、状态空间模型等）正在塑造着LLM的能力边界和应用前景。

**我将 LLM 训练生态总结为三大模块：领域前沿巨头、中国AI力量、小型实验室与非营利组织。**接下来一一介绍：

## **1\.领域前沿巨头**

第一梯队由资金极其雄厚、致力于开发最先进（SOTA）且主要为闭源模型的大型实验室构成。它们的战略通常是“全栈式”的，通过控制从研究、训练到部署的全过程来维持其技术领先地位和竞争优势。主要包括 **OpenAI、Google (含DeepMind)、Anthropic、Meta AI、xAI** 五家。

### OpenAI：引领行业变革，追求极致性能

在GPT-3的基础上，通过**指令微调（Instruction Tuning）和基于人类反馈的强化学习（RLHF）**，显著提升了模型的遵循指令和与人类价值观对齐的能力，最终推出了引发现象级应用的ChatGPT。尽管RLHF的概念早已存在，但OpenAI是第一家成功将其大规模应用于超大模型训练并取得惊人效果的公司。这一举措不仅极大提升了模型的实用性和可控性，也为整个行业**树立了新的技术范式**。OpenAI凭借ChatGPT的巨大成功，占据了强大的市场心智，拥有最广泛的用户基础和品牌认知度。**O1 推理模型的发布，也开启了新的后训练篇章。**

### Anthropic：以安全为基石，探索可靠的AI之路

由前OpenAI核心成员创立的Anthropic，从诞生之日起就将AI安全置于其使命的核心，致力于构建更可信、更可控、更符合人类价值观的AI系统。**“宪法AI”（Constitutional AI）是Anthropic在LLM训练领域最核心的贡献。**该方法通过让AI遵循一套预设的“宪法”（即一系列原则和价值观），在训练过程中进行自我监督和修正，从而减少对大规模、有偏见的人类标注数据的依赖，旨在从根本上提升模型的安全性和无害性。Anthropic的Claude3.5模型及最新的Claude4模型，**业内公认 coding 能力顶尖，已经成为广大程序员受众的首选模型。**此外，**2024年11月开源的模型上下文协议（MCP），定义了一种将 AI 助手连接到数据所在系统的新标准**，使开发者能够在其数据源与 AI 驱动工具之间建立安全的双向连接。

### Google：整合庞大资源，构建多模态与高效能的未来

作为AI领域的长期巨头和“Transformer”架构的诞生地，Google在过去三年中，整合其深厚的研究积累和强大的基础设施，以前所未有的规模和多模态能力，推动着大语言模型的发展。**Google拥有世界顶尖的AI研究团队（Google DeepMind）和为AI深度优化的计算基础设施（TPU），使其有能力进行更大规模、更前沿的模型训练。**Gemini 2.0及2.5的技术报告揭示了其战略重点：**原生多模态能力、超过100万token的超长上下文窗口**，以及为提升效率和实现高级推理而采用的MoE架构。

### Meta AI：开源运动的旗手，普惠AI的推动者

Meta AI通过其Llama系列模型的开源，掀起了一场席卷全球的AI普惠化运动，从根本上改变了顶级大模型技术只掌握在少数公司手中的格局。**Llama 的开源直接催生了一个庞大且活跃的生态系统。无数开发者基于Llama进行微调，创造出针对特定行业（如医疗、法律）和特定任务（如代码生成、多语言翻译）的优化模型。这验证了分布式、社区驱动的AI发展模式的可行性，加速了技术的迭代和创新。**Llama系列在技术上采用了如**SwiGLU激活函数**、**旋转位置编码（RoPE）和RMSNorm归一化**等先进架构，并在其发布论文中详细阐述，为整个学术界和工业界提供了宝贵的实践经验和研究方向。

### xAI：实时信息的探索者

由埃隆·马斯克创立，它的**核心贡献在于其模型**Grok**的设计理念——与实时信息流（即X平台，前身为Twitter）的深度整合。这打破了传统LLM依赖静态、历史数据集的训练模式。**Grok能够访问和处理最新的全球对话、新闻和趋势，使其在回答时事问题和追踪动态事件方面具有天然优势。

## **2\.中国AI力量**

一个至关重要且独特的生态系统，发展速度和创新能力不容小觑。不仅在追赶世界顶尖水平，更在开源生态、成本效益、代码能力和架构创新等多个维度上，贡献了独特的“中国智慧”。最近看到 Meta AI 挖角了数十位领域顶尖研究者，其中半数都是华人，由此可以看出，中国在顶尖人才储备方面具备巨大优势。

### DeepSeek：极致的性价比与开源代码能力

DeepSeek作为一家专注于基础大模型研究的创业公司，以其“技术极客”的形象和对开源的坚定投入，迅速在全球开发者社区中声名鹊起。DeepSeek最突出的贡献在于其对**混合专家（MoE）架构**的深度优化和应用，以及通过强化学习训练的推理模型**（DeepSeek R1）**。其**DeepSeek-V2**模型，创新性地采用了“深窄”网络结构和高效的专家路由机制。这使其在实现与顶级模型（如Llama 3 70B）相媲美性能的同时，激活的参数量却小得多，从而**极大地降低了推理成本**，让高性能模型的规模化应用变得更加可行。

DeepSeek不仅开源了性能强大的基础模型，还将其以非常友好的许可证（允许免费商用）提供给社区，并**开源了海量的、经过精心清洗的训练数据。**这种开放姿态极大地推动了全球AI研究的协同发展。

### 智谱AI（Zhipu AI）：从学术到产业，GLM架构的坚持与演进

脱胎于清华大学KEG实验室的智谱AI，是中国“学院派”力量的杰出代表。它基于其长期坚持的GLM架构，构建了从学术研究到产业应用的全链条能力。

智谱AI的核心贡献在于其自研的**通用语言模型（GLM）架构**。与主流的GPT（Decoder-only）架构不同，GLM早期采用了独特的双向注意力机制，并持续演进。从早期的GLM-130B到**ChatGLM**系列，再到最新的**GLM-4**，智谱AI证明了其架构路线的有效性和强大潜力。**2023年，智谱AI开源的ChatGLM-6B模型，以其在消费级显卡上即可流畅运行的低门槛和出色的中英文能力，点燃了国内大模型开源社区的火焰，成为无数开发者和研究者的“启蒙模型”。**

### 阿里巴巴（通义千问 Qwen）：体系化的开源与全面的能力布局

阿里巴巴依托其强大的云计算基础设施和丰富的业务场景，打造了“通义（Qwen）”系列大模型，并以一种“体系化、全方位”的开源策略，深刻影响了AI社区。

- **“全尺寸、全模态”的开源矩阵：** 阿里通义千问的开源是最大方的之一。它不仅开源了从18亿到720亿参数的多个尺寸模型（**Qwen系列**），还开源了强大的视觉语言模型（**Qwen-VL**）和音频理解模型（**Qwen-Audio**）。这种“全家桶”式开源，让不同需求的开发者都能找到合适的模型，**极大地丰富了开源生态**。

- **强大的云服务整合能力：** 通义大模型与其阿里云计算服务深度绑定，为企业客户提供从模型训练、微调到推理部署的一站式解决方案。

### 字节跳动（豆包/Seed）：强大的工程实力与数据飞轮

字节跳动凭借其在推荐算法领域积累的深厚工程能力和海量数据处理经验，后来居上，其大模型产品以惊人的速度在市场中占据一席之地。

- **极致的系统工程优化：** **字节跳动的核心贡献体现在其强大的系统工程和算法优化能力上。**其自研的**ByteScale等训练框架**，能够高效地利用大规模GPU集群进行训练。在模型侧，其**SEED系列模型**同样注重推理效率，通过MoE等架构，在保证性能的同时，致力于降低服务成本。

- **模型驱动的数据治理：** 字节跳动创新地将“模型”本身作为数据处理和筛选的核心工具。例如其Seed-Coder模型，**利用大模型来自动构建和清洗海量的代码训练数据，形成了一个高效的“数据飞轮”，实现了数据质量和模型能力的同步螺旋式上升。**

- **国民级应用的快速落地：** 字节将其大模型能力快速封装成**“豆包”**等产品，并借助其旗下抖音、今日头条等国民级App的巨大流量入口进行推广，迅速触达数以亿计的用户。这种“大力出奇迹”的应用推广能力是业界罕见的。

### 腾讯（混元）：全栈自研与场景驱动的融合

腾讯立足于其广泛的社交、游戏和企业服务生态，打造了“混元”大模型体系，强调技术的全链路自研和与业务场景的深度融合。

- **混合架构的探索与创新：** 腾讯混元在模型架构上进行了大胆的探索。其**Hunyuan-TurboS**模型，创新地**融合了Transformer和Mamba（一种状态空间模型）**，旨在结合Transformer强大的上下文理解能力和Mamba在处理长序列时的高效率，为平衡模型性能与推理成本提供了新的思路。

- **从底层算力到上层应用的全栈自研：** 腾讯强调从AI芯片、计算框架到模型算法的全链路自主可控，这为其模型的长期稳定迭代和与腾讯云服务的深度整合打下了坚实基础。

- **面向产业场景的深度优化：** 腾讯混元大模型在推出之初，就与其内部的腾讯会议、腾讯文档、企业微信等数百个业务进行了深度结合，针对产业互联网的需求进行了大量优化，例如提升模型在遵循复杂指令、生成格式化文本等方面的能力。

## **3\.小型实验室及非营利组织**

这些以研究为核心的机构在**培育开放、透明的AI生态系统**中扮演着不可或缺的角色。它们提供了大量的工具、数据和基础研究，为众多商业公司的发展奠定了基础。

### Prime Intellect：AI训练的去中心化革命

Prime Intellect旨在通过Web3和去中心化技术，从根本上改变AI模型的训练和所有权模式，挑战由少数云巨头垄断的算力格局。

- **构建去中心化AI训练协议：** Prime Intellect的核心贡献是其开发的**去中心化计算协议**。该协议能够将全球范围内闲置的GPU算力（从个人玩家到数据中心）连接起来，形成一个统一的、无需许可的P2P网络，共同训练大规模AI模型。

- **实现AI的集体所有权：** 在Prime Intellect的模式下，任何为模型训练贡献了算力或数据的参与者，都可以按贡献比例获得该模型的所有权（通常以代币形式体现）。这颠覆了由单一公司拥有和控制模型的传统模式，旨在实现AI成果的“集体共创、共同拥有”。

- **降低AI的准入门槛和成本：** 通过**聚合全球的廉价或闲置算力，该平台有望大幅降低训练先进AI模型的成本，让更多初创公司和独立研究者也能参与到前沿AI的开发中来。**其已经成功组织了多个模型的去中心化训练。

### Liquid AI：挑战Transformer，探索下一代AI架构

Liquid AI由MIT CSAIL的知名教授Daniela Rus和Ramin Hasani创立，致力于将一种全新的神经网络架构商业化，挑战自2017年以来统治AI领域的Transformer架构。

- **开创“液态神经网络”（LNNs）：** Liquid AI的核心贡献是其研发的**液态神经网络（Liquid Neural Networks）**。这是一种受生物神经系统启发的连续时间（time-continuous）神经网络。与Transformer处理离散化数据（token）的方式不同，LNNs通过一组微分方程来建模世界，使其在处理连续、动态变化的数据流时更具优势。

- **追求更高的效率和因果推理：** LNNs的参数量比同等性能的Transformer模型小一个数量级，这使得它们在推理时能耗更低、速度更快。更重要的是，**由于其动态特性，LNNs在理解系统动态和因果关系方面被认为具有巨大潜力，有望解决当前LLM在真正理解物理世界和进行复杂推理方面的短板。**

- **为边缘计算和具身智能提供新路径：** 由于其小巧和高效的特性，LNNs非常适合部署在资源受限的边缘设备上，如自动驾驶汽车、机器人和无人机，为实现更强大、更自主的具身智能开辟了新的可能性。

### Pleias：开放科学与开放数据集的倡导者

- **推动公共数据集协作：** Pleias与Hugging Face、EleutherAI等组织合作，参与协调了**Common Corpus**等公共领域数据集的创建，致力于为AI模型训练提供更多开放、合规的数据来源。

- **倡导“AI向善”与特定领域应用：** 其关联项目（如医疗领域的Asclepius模型）体现了利用AI解决特定社会问题的目标，强调AI技术的正面社会影响，并探索在隐私敏感领域（如医疗）使用合成数据进行训练的可行性。

Pleias的贡献更侧重于理念倡导和生态协作，是推动AI向更开放、更负责任方向发展的重要参与者之一。

### Hugging Face：AI领域的“GitHub”，开源生态的构建者与核心

如果说Llama等模型是开源运动的“火种”，那么Hugging Face就是将这些火种汇聚、分发并燃成燎原之火的“风”与“平台”。它对LLM训练领域的贡献是生态级和设施级的。

- **打造行业标准工具链：** Hugging Face的`transformers`**库是其最核心的贡献。**它将不同机构发布的、架构各异的大模型（如BERT, GPT, T5, Llama）封装在统一、易用的API之下，极大地降低了开发者使用、微调和部署模型的门槛。此外，其`datasets`、`evaluate`和`tokenizers`等库共同构成了一个完整的、标准化的LLM工作流。

- **构建全球最大的AI社区与模型中心（Hub）：** Hugging Face Hub已成为全球开发者分享和发现AI模型、数据集和演示应用（Spaces）的中心枢纽。这种集中化的平台极大促进了知识的传播、协作的产生和技术的可复现性，让任何开发者都能站在巨人的肩膀上。

- **推动AI的民主化与透明化：** 通过“模型名片（Model Cards）”等工具，Hugging Face倡导对模型的训练细节、局限性和偏见进行更透明的说明。它积极拥抱并推动所有开源模型的传播，使顶尖AI技术不再是少数巨头的专利，让全球的开发者和研究者都能参与到这场技术革命中。

### EleutherAI：开源大模型的“普罗米修斯”，点燃星星之火

在Meta的Llama出现之前，EleutherAI这个由独立研究者组成的松散组织，是全球开源大模型运动真正的先驱。他们证明了，在没有国家或顶级公司支持的情况下，通过协作同样可以构建出强大的语言模型。

- **早期开源模型的开拓者：** 在大模型被视为少数公司的时代，EleutherAI开发并发布了**GPT-Neo**和**GPT-J**系列模型。这些模型是当时世界上性能最强、规模最大的开源语言模型，首次让公众能够自由地使用和研究接近GPT-3级别的技术。

- **构建里程碑式的数据集“The Pile”：** 认识到高质量数据的重要性，EleutherAI组织并开源了**The Pile——一个精心筛选、规模高达825GB的多样化文本数据集。The Pile成为后续众多大模型（包括许多商业模型）训练和研究的基石，并推动了AI社区对数据透明度和质量的重视。**

- **培养开放研究文化：** EleutherAI完全在一个公开的Discord服务器上进行协调，其研究和开发过程高度透明。这种激进的开放模式，不仅吸引了全球顶尖的志愿者，也为后续的开源AI项目（如Stable Diffusion）树立了榜样。

### Allen Institute for AI (AI2)：倡导“AI为共同利益”的开放科学典范

由微软联合创始人保罗·艾伦创建的非盈利研究机构AI2，始终致力于推动AI领域的开放研究，以期让AI的成果惠及全人类。

- **完全透明的开源模型（OLMo）：** AI2最核心的贡献是其\*\*OLMo (Open Language Model)\*\*项目。与当时其他只发布模型权重和代码的开源模型不同，OLMo真正践行了“开放科学”的理念，**开源了构建一个大模型所需的全套资源，包括完整的训练数据、训练日志、评估基准和所有中间检查点。这种前所未有的透明度，让研究社区能够深入剖析大模型的训练动态，进行可复现的研究。**

- **高质量数据集与工具：** AI2持续产出高质量的学术数据集（如Dolma，OLMo的训练数据集）和评估工具，为AI研究社区提供了宝贵的公共产品，帮助研究者更好地理解和评估模型能力。

---

两种主流开发模式——开放与封闭——各有其战略优劣，并可能形成长期共存的局面。

- **闭源模型:** 能够提供最顶尖的性能和可控、安全的环境，这为愿意支付高价的企业客户构筑了强大的护城河。然而，它们也面临着被庞大的全球开源社区在创新速度上超越的风险。

- **开源模型:** 受益于社区驱动的快速创新、高透明度和低成本，从而获得了广泛的采用。但它们在商业化变现和确保大规模负责任使用方面面临挑战。

这并非一个“赢者通吃”的市场，而是一个可持续的僵局。两大生态系统将相互依存、相互促进。开源模型将继续在原始能力上迅速追赶，迫使封闭模型巨头在安全性、可靠性和集成的全栈解决方案上寻求差异化竞争优势。

而且，开源与开放的训练生态，让更多人能有机会参与这场革命，是一件很棒的事。


> PS：本文由 Gemini 2.5 Pro 协作完成