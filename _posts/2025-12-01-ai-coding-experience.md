---
title: AI coding 实践的一点经验
description: 人如何更好地与 AI 协作，始终是一个核心问题。
date: 2025-12-01 23:30:00 +0800
categories: [AI, reflection]
tags: [practice]
---


三个月前，我开始在社交平台刷到 Claude Code 的资讯，许多程序员都在说它很牛。我抱着好奇的心想要体验一下，9月7日，我成功注册并启用了 Claude Code，三天之后，**花了两个小时 vibe 出了第一个能成功跑的小产品：一个将视频自动总结成文档的工具。**

### 我的开发过程是这样的：

1. 和 Gemini 聊需求，要多聊几轮，聊到足够详细。并且投喂一些可以参考的资料或者代码仓库作为上下文。**让 AI 生成一份非常清晰的项目需求文档。**

2. 创建一个空文件夹，把项目需求文档放进去。启动 Claude Code，让它详细阅读文档，并开始构建项目。

3. 中间会遇到一些问题反馈，需要人工介入解决。这次主要遇到几个问题：
    - YouTube 视频需要验证才能下载，最终的办法是获取浏览器 cookies 存在本地文件夹里。
    - 本地 Ollama 模型没有 TTS 工具，只能调用 Gemini 或者 OpenAI 的 API 进行文字转录。于是需要保存支持 TTS 大模型的 API 到环境中，供 Claude Code 读取。
    - 本地因为开了网络代理，导致 Ollama 一直连接不成功，Claude Code 花了好几轮测试发现了这个问题，并修订代码绕开代理，成功连接。

4. 解决完所有问题后，执行自动测试，测试调试成功。

5. 将整个项目发布到 GitHub 仓库。

**现在回过头来看，还是感到不可思议，AI coding 居然已经强到如此地步。**

### 人如何更好地与 AI 协作，始终是一个核心问题。

后续开发过程也遇到过一些问题，比如：<u>已实现且调试好的功能模块，在增加新功能时，AI 仿佛失忆了，会重新做一些已经完成的工作，甚至反复犯一些之前犯过的错误。</u>

如何才能避免出现类似的不可控情况呢？有几个方法：

- **使用更强的模型。**当任务复杂或AI出现“倒退”现象时，果断开启高算力模式。比如可以在 Cloud Code 中使用 Ultrathink 指令。

- **让 AI 先理解一遍代码结构。**在修改前，先让AI总结现有代码的结构。然后，使用AI总结出的术语来下达指令，可以极大地增强指令的精确性，避免误解。

- **定期生成和维护技术文档。**对于项目中稳定、核心的模块（如登录、认证），或者曾经花费大量时间才解决的复杂 Bug，可以让 AI 将其解决方案和技术设计固化为 Markdown 文档。当开启新项目或AI出现“遗忘”时，让它首先阅读这些文档。

- **清除上下文。**当 AI 在一个长对话中反复犯错时，很可能是受到了错误上下文的污染。清除或重置会话，只针对当前问题进行提问，有时能奇迹般地解决问题。 

- **打印日志记录。**让 AI 在代码的每一步关键流程中，都插入 print 或 log 语句，输出中间变量和状态。

- **让 AI 自己反思。**将 AI 生成的 Log 日志、报错细节，原封不动地复制粘贴回给 AI ，让它自己分析日志、定位问题。

作为一个新手开发者，我感觉到自己<u>时常还会遇到一个关键问题就是【不知道自己不知道】，也就是缺乏先验知识。</u>

比如10月份，我在开发一个浏览器插件，用于导出推文为海报，AI 默认使用了 html to canvas 的技术，但这个技术本身有局限性，以至于最终呈现并不如预期。这大概是前端工程师都知道的知识。但因为我以前没做过前端开发，因此也不知道。结果耗费了许久，才发现这个问题。中间 AI 一直尝试各种 Debug，都是无效操作，没能找到关键问题。

复盘之后，我发现了两个要点：


- **如果我能一开始就让 AI 从更全面的角度思考和计划。**比如从下面 4 个维度逐项盘点：
   

   | 维度 | 让 AI 回答的关键问题 | 典型提示句 | 
   |---|---|---|
   | **目标与约束** | “我想达到什么效果？有什么性能、兼容、耗时、预算限制？” | “请列出实现 XXX 时最常见的硬约束和软约束。” | 
   | **方案全景** | “业界在解决类似问题时，主流方案/库/范式有哪些？各自局限？” | “给我一个技术选型矩阵，横向比较常见方法的优缺点。” | 
   | **已知雷区** | “有哪些经验丰富的开发者默认都会规避的坑？” | “html-to-canvas 类方案有哪些天生限制？在什么场景下会翻车？” | 
   | **最小可行验证** | “如果我要尽快验证可行性，最小的 PoC 长什么样？” | “用最短代码演示能否跑通 X，同时输出评估指标。” | 
   

   这样子在开始开发前，就能获得一张“可能缺什么”的雷达图；**AI 会把你没想到的问题（未知未知）显式化为待确认的清单。**

- 如果我能增强一点意识，发现 AI 陷入困境，**及时制止。然后重启对话**，让 AI 重新阅读代码并反思可能存在的问题点。那么，也有机会可以更快地解决。


通过这些实践，我发现自己在与 AI 协作时，还是存在一些思维误区，导致了低效和犯错。

有时，仅提供报错和差异的截图是远远不够的，**AI 也有自己的局限，也会陷入自己的怪圈。**

这时，就该人类上场了。**人类需要发挥自己的认知优势，通过更高阶的思维方式，帮助 AI 定位到关键问题，才能提高开发效率。**
